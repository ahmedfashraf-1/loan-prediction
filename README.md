# ðŸ“Š Loan Defaulter Prediction

## 1. ðŸ“– Introduction

This repository contains a complete end-to-end project for predicting loan default risk using machine learning. The dataset includes both current application data and historical application records which are used for feature engineering, exploratory data analysis, model training, and inference.

**Files included in the dataset:**

* `application_data.csv` â€” current client-level features and the `TARGET` label (0 = repaid, 1 = default).
* `previous_application.csv` â€” historical loan applications for the same clients (use to create aggregated features per `SK_ID_CURR`).
* `columns_description.csv` â€” a dictionary describing column meanings and units.

---

## 2. ðŸ’» Contributors

| Name            | GitHub                                                                     |
| --------------- | -------------------------------------------------------------------------- |
| Ahmed Ashraf    | [https://github.com/ahmedfashraf-1](https://github.com/ahmedfashraf-1)     |
| Malak Ahmed     | [https://github.com/Malak-A7med](https://github.com/Malak-A7med)           |
| Tasneem Hussein | [https://github.com/tasneemhussein12](https://github.com/tasneemhussein12) |
| Mohamed Sheta   | [https://github.com/Mohamed-Sheta](https://github.com/Mohamed-Sheta)       |
| Ossama Ayman    | [https://github.com/Ossama-Ayman](https://github.com/Ossama-Ayman)         |

---

## 3. ðŸŽ¯ Project Objective

To build robust ML models that estimate the probability of loan default by combining current application features with aggregated historical behaviour from previous applications. The resulting models and UI are intended to help data scientists and risk analysts inspect predictions and understand the most important risk drivers.

---

## 4. ðŸ“‚ Dataset Overview

Use the three CSVs together during EDA and feature engineering. The `columns_description.csv` file is the authority for field meanings.

> Column description (stored in repo): `data/columns_description.csv`

---

## 5. ðŸ—ï¸ Project Structure

```
Loan-Defaulter-Prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ application_data.csv
â”‚   â”œâ”€â”€ previous_application.csv
â”‚   â””â”€â”€ columns_description.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_and_modeling.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ final_model.pkl
â”‚
â”œâ”€â”€ fast_api_app/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ assets/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**What each folder contains:**

* `data/` â€” raw CSVs (do **not** push sensitive data to public repos).
* `notebooks/` â€” notebooks for EDA, preprocessing, feature engineering and training experiments.
* `models/` â€” serialized model(s) and preprocessor objects used by the API/UI.
* `fast_api_app/` â€” FastAPI backend exposing prediction endpoints.
* `streamlit/` â€” Streamlit frontend for interactive model exploration.
* `requirements.txt` â€” pinned Python dependencies.

---

## 6. âš™ï¸ Installation

Recommended: create and activate a virtual environment first.

```bash
# create & activate venv (example for Windows PowerShell)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# or on macOS / Linux
python3 -m venv .venv
source .venv/bin/activate
```

Install the project requirements:

```bash
pip install -r requirements.txt
```

> Tip: `requirements.txt` should contain packages such as `pandas`, `numpy`, `scikit-learn`, `joblib`/`pickle` (for model I/O), `fastapi`, `uvicorn`, and `streamlit`. Adjust pins to match your environment.

---

## 7. ðŸš€ Running the Applications

You can run the backend API and the Streamlit frontend locally. They should be launched in separate terminals (or as background processes).

### A) Run the Streamlit App

```bash
cd streamlit
streamlit run app.py
```

Streamlit will automatically open a browser window (or show a local URL like `http://localhost:8501`).

If the Streamlit UI expects the locally-running FastAPI backend, make sure the API is running before interacting with the UI.

### B) Run the FastAPI Backend

```bash
cd fast_api_app
uvicorn fast_api_app.main:app --reload
```

This binds by default to `127.0.0.1:8000`. Open the interactive API docs at `http://127.0.0.1:8000/docs`.

**Notes:**

* `--reload` enables auto-reload for development. Remove it in production.
* If your `main.py` exposes the app with a different variable name or module path, replace `fast_api_app.main:app` accordingly.

### C) Running both in parallel

Open two terminals/tabs and run the Streamlit command in one and the Uvicorn command in the other. Alternatively use a process manager (tmux, GNU screen) or Docker (see optional section below).

---

## 8. ðŸ§­ Usage & Workflow

1. Put the CSVs in the `data/` folder.
2. Open `notebooks/eda_and_modeling.ipynb` and run the cells to reproduce preprocessing, feature engineering, and model training steps.
3. After training, save the model and any preprocessing pipeline into `models/` (e.g., `final_model.pkl`).
4. Update `fast_api_app/main.py` to load the model from `models/` and expose an inference endpoint (e.g., `/predict`).
5. Start the FastAPI server and the Streamlit app to serve predictions.

---

## 9. ðŸ“š References & Resources

* **Data Source:** Kaggle â€” Loan Defaulter Dataset
* **UI/UX Design:** [https://shaper-dark-muse.lovable.app/](https://shaper-dark-muse.lovable.app/)

---
